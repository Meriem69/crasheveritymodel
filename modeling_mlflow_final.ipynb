{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Pr√©diction de la Gravit√© des Accidents Routiers ‚Äî MLflow & MLOps\n",
    "\n",
    "**Projet** : Int√©gration de MLflow pour le tracking et l'optimisation des mod√®les de machine learning  \n",
    "**Dataset** : Donn√©es BAAC (Bulletin d'Analyse des Accidents Corporels de la route)  \n",
    "**Objectif** : Pr√©dire la gravit√© d'un accident (4 classes) et comparer plusieurs mod√®les de classification\n",
    "\n",
    "---\n",
    "\n",
    "## Qu'est-ce que MLflow ?\n",
    "\n",
    "**MLflow** est un outil open-source qui r√©sout un probl√®me concret : quand on entra√Æne plusieurs mod√®les avec des param√®tres diff√©rents, on perd rapidement la trace des r√©sultats.\n",
    "\n",
    "**Analogie** : c'est le carnet de recettes automatique du data scientist. √Ä chaque cuisson (entra√Ænement), il note les ingr√©dients (param√®tres), le r√©sultat (m√©triques), et garde une photo du g√¢teau (artefacts).\n",
    "\n",
    "### Les 4 composants de MLflow\n",
    "\n",
    "| Composant | R√¥le | Analogie |\n",
    "|-----------|------|----------|\n",
    "| **Tracking** | Enregistre param√®tres, m√©triques, artefacts de chaque entra√Ænement | Le carnet de notes |\n",
    "| **Models** | Format standard pour sauvegarder les mod√®les | La bo√Æte de conservation |\n",
    "| **Registry** | Entrep√¥t versionn√© des meilleurs mod√®les | La biblioth√®que officielle |\n",
    "| **Projects** | Packager le code ML pour le reproduire sur n'importe quelle machine | La recette imprimable |\n",
    "\n",
    "### Vocabulaire cl√©\n",
    "\n",
    "- **Run** : un entra√Ænement = une entr√©e dans le carnet\n",
    "- **Experiment** : un groupe de runs li√©s au m√™me sujet\n",
    "- **Param√®tre** (`log_param`) : un r√©glage choisi AVANT l'entra√Ænement\n",
    "- **M√©trique** (`log_metric`) : un r√©sultat mesur√© APR√àS l'entra√Ænement\n",
    "- **Artefact** (`log_artifact`) : un fichier produit pendant l'entra√Ænement (image, mod√®le...)\n",
    "- **Tracking URI** : l'adresse o√π MLflow √©crit ses notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ‚öôÔ∏è √âTAPE 0 ‚Äî Installation des librairies\n\nInstallation de toutes les librairies n√©cessaires en une seule commande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de toutes les librairies n√©cessaires en une seule commande\n",
    "# -q = quiet : pas d'affichage des logs d'installation\n",
    "!pip install mlflow lightgbm xgboost optuna -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ √âTAPE 1 ‚Äî Imports des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, accuracy_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "\n",
    "# Suppression des avertissements non critiques\n",
    "logging.getLogger(\"mlflow.utils.environment\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"mlflow.lightgbm\").setLevel(logging.ERROR)\n",
    "os.environ[\"GIT_PYTHON_REFRESH\"] = \"quiet\"\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ √âTAPE 2 ‚Äî Chargement des donn√©es\n",
    "\n",
    "Le dataset contient des donn√©es sur les accidents de la route en France (donn√©es BAAC).  \n",
    "Chaque ligne = une personne impliqu√©e dans un accident.\n",
    "\n",
    "**Variable cible `grav` :** 1=Indemne | 2=Bless√© l√©ger | 3=Hospitalis√© | 4=Tu√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents = pd.read_csv('data/df_accidents_clean.csv')\n",
    "\n",
    "print(f\"Dimensions : {df_accidents.shape[0]:,} lignes √ó {df_accidents.shape[1]} colonnes\")\n",
    "df_accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_copy = df_accidents.copy()\n",
    "\n",
    "# Visualisation de la r√©partition des classes\n",
    "class_labels = {1: 'Indemne', 2: 'Bless√© l√©ger', 3: 'Hospitalis√©', 4: 'Tu√©'}\n",
    "class_counts = df_accidents_copy['grav'].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = ['#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\n",
    "bars = ax.bar(\n",
    "    [class_labels[i] for i in class_counts.index],\n",
    "    class_counts.values,\n",
    "    color=colors\n",
    ")\n",
    "ax.set_title('R√©partition des classes de gravit√©', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Nombre de cas')\n",
    "for bar, val in zip(bars, class_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,\n",
    "            f'{val:,}\\n({val/len(df_accidents_copy)*100:.1f}%)',\n",
    "            ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=100)\n",
    "plt.show()\n",
    "print(\"‚ö†Ô∏è  D√©s√©quilibre visible : classe 2 (Bless√© l√©ger) tr√®s sous-repr√©sent√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÇÔ∏è √âTAPE 3 ‚Äî Pr√©paration des donn√©es\n",
    "\n",
    "**S√©paration X / y :**\n",
    "- **X** = toutes les colonnes sauf `grav` ‚Üí ce qu'on donne au mod√®le\n",
    "- **y** = colonne `grav` ‚Üí ce qu'on veut pr√©dire\n",
    "\n",
    "**Train / Test Split :**\n",
    "- 80% des donn√©es pour apprendre\n",
    "- 20% des donn√©es pour √©valuer (donn√©es que le mod√®le n'a jamais vues)\n",
    "- `stratify=y` : conserve les proportions des 4 classes dans chaque ensemble\n",
    "\n",
    "**Standardisation :**  \n",
    "Ram√®ne toutes les colonnes √† la m√™me √©chelle (moyenne=0, √©cart-type=1).  \n",
    "`fit_transform` sur le train, `transform` seulement sur le test (jamais recalculer sur le test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration X / y\n",
    "y = df_accidents_copy['grav']\n",
    "X = df_accidents_copy.drop(columns=['grav'])\n",
    "\n",
    "# Train / Test Split (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Apprend ET applique\n",
    "X_test_scaled  = scaler.transform(X_test)       # Applique seulement\n",
    "\n",
    "print(f\"Train : {X_train.shape[0]:,} lignes | Test : {X_test.shape[0]:,} lignes\")\n",
    "print(f\"Features : {X.shape[1]} colonnes\")\n",
    "print(\"‚úÖ Donn√©es pr√™tes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ √âTAPE 4 ‚Äî Configuration des 4 mod√®les\n",
    "\n",
    "Les 4 mod√®les sont tous des **classificateurs** : ils pr√©disent une cat√©gorie parmi 4.\n",
    "\n",
    "| Mod√®le | Principe | Analogie |\n",
    "|--------|----------|----------|\n",
    "| **LogisticRegression** | Calcul math√©matique lin√©aire simple | Le math√©maticien |\n",
    "| **RandomForest** | Vote de 800 arbres de d√©cision ind√©pendants | Le jury populaire |\n",
    "| **XGBoost** | Arbres qui apprennent des erreurs du pr√©c√©dent | L'√©l√®ve perfectionniste |\n",
    "| **LightGBM** | M√™me id√©e qu'XGBoost, optimis√© pour la vitesse | L'√©l√®ve perfectionniste rapide |\n",
    "\n",
    "**Param√®tres cl√©s :**\n",
    "- `n_estimators=800` : 800 arbres ‚Äî plus d'arbres = meilleure pr√©cision mais plus lent\n",
    "- `max_depth=30` : profondeur max des arbres ‚Äî profond = capture plus de nuances\n",
    "- `learning_rate=0.01` : vitesse d'apprentissage ‚Äî lent = plus pr√©cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des 4 mod√®les avec les param√®tres optimis√©s\n",
    "model_configs = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression,\n",
    "        \"params\": {\n",
    "            \"max_iter\": 1000,           # Nombre max d'it√©rations pour converger\n",
    "            \"class_weight\": \"balanced\", # Compense le d√©s√©quilibre des classes\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 800,               # 800 arbres de d√©cision\n",
    "            \"max_depth\": 30,                   # Profondeur max de chaque arbre\n",
    "            \"min_samples_split\": 5,            # Min d'exemples pour diviser un n≈ìud\n",
    "            \"min_samples_leaf\": 2,             # Min d'exemples dans une feuille\n",
    "            \"max_features\": \"sqrt\",            # Nb de features par split\n",
    "            \"class_weight\": \"balanced_subsample\",\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1                       # Utilise tous les c≈ìurs CPU\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 800,\n",
    "            \"max_depth\": 30,\n",
    "            \"learning_rate\": 0.01,             # Taux d'apprentissage\n",
    "            \"subsample\": 0.8,                  # 80% des donn√©es par arbre\n",
    "            \"colsample_bytree\": 0.8,           # 80% des colonnes par arbre\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"LightGBM\": {\n",
    "        \"model\": LGBMClassifier,\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 800,\n",
    "            \"max_depth\": 30,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ {len(model_configs)} mod√®les configur√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä √âTAPE 5 ‚Äî Configuration MLflow\n",
    "\n",
    "**`set_tracking_uri`** : indique √† MLflow o√π stocker les donn√©es.  \n",
    "Ici, dans un fichier SQLite local (`mlflow.db`) ‚Äî pas besoin de serveur s√©par√© sur Colab.\n",
    "\n",
    "**`set_experiment`** : cr√©e un dossier logique pour regrouper les runs.  \n",
    "Tous les 4 mod√®les du Jour 1 iront dans l'exp√©rience `crashseveritymodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O√π MLflow stocke ses donn√©es (fichier local SQLite)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "\n",
    "# Nom de l'exp√©rience ‚Äî regroupe tous les runs des 4 mod√®les\n",
    "mlflow.set_experiment(\"crashseveritymodel\")\n",
    "\n",
    "print(\"‚úÖ MLflow configur√©\")\n",
    "print(\"   Tracking URI : sqlite:///mlflow.db\")\n",
    "print(\"   Exp√©rience   : crashseveritymodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèãÔ∏è √âTAPE 6 ‚Äî Entra√Ænement des 4 mod√®les avec tracking MLflow\n",
    "\n",
    "Pour chaque mod√®le, la boucle :\n",
    "1. Ouvre un **run MLflow** (`with mlflow.start_run`)\n",
    "2. Entra√Æne le mod√®le (`model.fit`)\n",
    "3. Fait des pr√©dictions (`model.predict`)\n",
    "4. Calcule les m√©triques\n",
    "5. Logue tout dans MLflow\n",
    "6. G√©n√®re et logue la matrice de confusion comme artefact\n",
    "\n",
    "**Pourquoi LabelEncoder pour XGBoost ?**  \n",
    "XGBoost exige des classes num√©rot√©es de 0 √† N-1.  \n",
    "Notre variable `grav` va de 1 √† 4 ‚Üí LabelEncoder convertit [1,2,3,4] ‚Üí [0,1,2,3] avant l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les r√©sultats et les afficher ensuite\n",
    "results = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "\n",
    "    print(f\"\\n‚è≥ Entra√Ænement de {model_name}...\")\n",
    "\n",
    "    # Ouverture d'un run MLflow ‚Äî chaque mod√®le = une entr√©e dans le carnet\n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "\n",
    "        # Instanciation du mod√®le avec ses param√®tres\n",
    "        model = config[\"model\"](**config[\"params\"])\n",
    "\n",
    "        # Logging des hyperparam√®tres AVANT l'entra√Ænement\n",
    "        mlflow.log_params(config[\"params\"])\n",
    "\n",
    "        # Cas particulier XGBoost : besoin de classes num√©rot√©es 0 √† N-1\n",
    "        if model_name == \"XGBoost\":\n",
    "            le = LabelEncoder()\n",
    "            y_train_enc = le.fit_transform(y_train)  # [1,2,3,4] ‚Üí [0,1,2,3]\n",
    "            model.fit(X_train_scaled, y_train_enc)\n",
    "            y_pred = le.inverse_transform(model.predict(X_test_scaled))  # Retour [1,2,3,4]\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Calcul des m√©triques\n",
    "        accuracy  = accuracy_score(y_test, y_pred)\n",
    "        report    = classification_report(y_test, y_pred, output_dict=True)\n",
    "        f1        = report[\"weighted avg\"][\"f1-score\"]\n",
    "        precision = report[\"weighted avg\"][\"precision\"]\n",
    "        recall    = report[\"weighted avg\"][\"recall\"]\n",
    "\n",
    "        # Logging des m√©triques dans MLflow\n",
    "        mlflow.log_metric(\"accuracy\",           accuracy)\n",
    "        mlflow.log_metric(\"f1_weighted\",        f1)\n",
    "        mlflow.log_metric(\"precision_weighted\", precision)\n",
    "        mlflow.log_metric(\"recall_weighted\",    recall)\n",
    "\n",
    "        # Sauvegarde des r√©sultats pour les graphiques\n",
    "        results[model_name] = {\n",
    "            \"accuracy\": accuracy, \"f1\": f1,\n",
    "            \"precision\": precision, \"recall\": recall,\n",
    "            \"y_pred\": y_pred, \"model\": model\n",
    "        }\n",
    "\n",
    "        # G√©n√©ration et logging de la matrice de confusion\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.matshow(cm, cmap=\"Blues\", alpha=0.6)\n",
    "        for (i, j), val in np.ndenumerate(cm):\n",
    "            ax.text(j, i, f'{val:,}', ha='center', va='center', color='red', fontsize=9)\n",
    "        ax.set_title(f\"Confusion Matrix - {model_name}\\nAccuracy: {accuracy:.2%}\")\n",
    "        ax.set_xlabel('Pr√©dit')\n",
    "        ax.set_ylabel('R√©el')\n",
    "        ax.set_xticklabels(['', 'Indemne', 'Bless√© l√©ger', 'Hospitalis√©', 'Tu√©'])\n",
    "        ax.set_yticklabels(['', 'Indemne', 'Bless√© l√©ger', 'Hospitalis√©', 'Tu√©'])\n",
    "        cm_file = f\"confusion_matrix_{model_name}.png\"\n",
    "        plt.savefig(cm_file, bbox_inches='tight')\n",
    "        mlflow.log_artifact(cm_file)  # Envoi du fichier dans MLflow\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Sauvegarde du mod√®le dans MLflow\n",
    "        if model_name == \"XGBoost\":\n",
    "            mlflow.xgboost.log_model(model, name=model_name + \"_accidents_model\")\n",
    "            model_uri = f\"runs:/{run.info.run_id}/XGBoost_accidents_model\"\n",
    "            mlflow.register_model(model_uri, \"XGBoost_accidents_model\")\n",
    "        elif model_name == \"LightGBM\":\n",
    "            mlflow.lightgbm.log_model(model, name=model_name + \"_accidents_model\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(sk_model=model, name=model_name + \"_accidents_model\")\n",
    "\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "\n",
    "        print(f\"‚úÖ {model_name} ‚Üí Accuracy: {accuracy:.2%} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ Tous les mod√®les entra√Æn√©s et logg√©s dans MLflow !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä √âTAPE 7 ‚Äî Visualisation et comparaison des 4 mod√®les\n",
    "\n",
    "Ces graphiques permettent de comparer visuellement les performances des mod√®les ‚Äî exactement ce qu'on voit dans l'UI MLflow, mais directement dans le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 1 : Comparaison des m√©triques par mod√®le ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "model_names  = list(results.keys())\n",
    "accuracies   = [results[m][\"accuracy\"]  for m in model_names]\n",
    "f1_scores    = [results[m][\"f1\"]        for m in model_names]\n",
    "precisions   = [results[m][\"precision\"] for m in model_names]\n",
    "recalls      = [results[m][\"recall\"]    for m in model_names]\n",
    "\n",
    "x     = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "colors_metrics = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - 1.5*width, accuracies,  width, label='Accuracy',  color=colors_metrics[0], alpha=0.85)\n",
    "bars2 = ax.bar(x - 0.5*width, f1_scores,   width, label='F1 weighted', color=colors_metrics[1], alpha=0.85)\n",
    "bars3 = ax.bar(x + 0.5*width, precisions,  width, label='Precision', color=colors_metrics[2], alpha=0.85)\n",
    "bars4 = ax.bar(x + 1.5*width, recalls,     width, label='Recall',    color=colors_metrics[3], alpha=0.85)\n",
    "\n",
    "# Afficher les valeurs sur les barres\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                f'{bar.get_height():.2%}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Mod√®le')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des 4 mod√®les ‚Äî Toutes m√©triques', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.set_ylim(0, 0.85)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_all_metrics.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 2 : Classement par accuracy (du plus faible au meilleur) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "sorted_models = sorted(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "names_sorted  = [m for m, _ in sorted_models]\n",
    "acc_sorted    = [r['accuracy'] for _, r in sorted_models]\n",
    "colors_rank   = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "bars = ax.barh(names_sorted, acc_sorted, color=colors_rank, alpha=0.85, height=0.5)\n",
    "\n",
    "for bar, val in zip(bars, acc_sorted):\n",
    "    ax.text(val + 0.002, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.2%}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Classement des mod√®les par accuracy\\n(du moins bon au meilleur)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, max(acc_sorted) + 0.08)\n",
    "ax.axvline(x=max(acc_sorted), color='green', linestyle='--', alpha=0.5, label=f'Meilleur : {max(acc_sorted):.2%}')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_ranking.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 3 : 4 matrices de confusion c√¥te √† c√¥te ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class_labels_short = ['Indemne', 'Bless√©\\nl√©ger', 'Hospitalis√©', 'Tu√©']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    cm = confusion_matrix(y_test, res['y_pred'])\n",
    "\n",
    "    # Normalisation pour voir les pourcentages\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    im = axes[i].imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
    "\n",
    "    # Afficher valeurs absolutes ET pourcentages\n",
    "    for row in range(cm.shape[0]):\n",
    "        for col in range(cm.shape[1]):\n",
    "            color = 'white' if cm_norm[row, col] > 0.5 else 'black'\n",
    "            axes[i].text(col, row,\n",
    "                        f'{cm[row, col]:,}\\n({cm_norm[row, col]:.0%})',\n",
    "                        ha='center', va='center', color=color, fontsize=8)\n",
    "\n",
    "    axes[i].set_xticks(range(4))\n",
    "    axes[i].set_yticks(range(4))\n",
    "    axes[i].set_xticklabels(class_labels_short, fontsize=9)\n",
    "    axes[i].set_yticklabels(class_labels_short, fontsize=9)\n",
    "    axes[i].set_xlabel('Pr√©dit', fontsize=10)\n",
    "    axes[i].set_ylabel('R√©el',   fontsize=10)\n",
    "    axes[i].set_title(\n",
    "        f'{model_name}\\nAccuracy: {res[\"accuracy\"]:.2%} | F1: {res[\"f1\"]:.3f}',\n",
    "        fontsize=11, fontweight='bold'\n",
    "    )\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.suptitle('Matrices de confusion ‚Äî Comparaison des 4 mod√®les\\n(diagonale = bonnes pr√©dictions)',\n",
    "             fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_all.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚ö†Ô∏è  La classe 2 (Bless√© l√©ger) est la plus difficile √† pr√©dire pour tous les mod√®les\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß √âTAPE 8 ‚Äî Tuning manuel de LightGBM\n",
    "\n",
    "LightGBM est le meilleur mod√®le. On va tester 3 configurations de param√®tres choisies manuellement.\n",
    "\n",
    "**Tuning manuel** = on choisit nous-m√™mes les combinaisons √† tester, en s'appuyant sur notre intuition.  \n",
    "Chaque configuration = **1 run MLflow s√©par√©** dans une exp√©rience d√©di√©e `tuning-lightgbm`.  \n",
    "Pourquoi une exp√©rience s√©par√©e ? Pour ne pas m√©langer les runs de comparaison avec les runs de tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"tuning-lightgbm\")\n",
    "\n",
    "tuning_configs = [\n",
    "    {\"n_estimators\": 200, \"max_depth\": 8,  \"learning_rate\": 0.05},  # Rapide\n",
    "    {\"n_estimators\": 300, \"max_depth\": 10, \"learning_rate\": 0.03},  # √âquilibr√©e\n",
    "    {\"n_estimators\": 500, \"max_depth\": 12, \"learning_rate\": 0.01},  # Profonde\n",
    "]\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "for i, params in enumerate(tuning_configs):\n",
    "    with mlflow.start_run(run_name=f\"LightGBM-config-{i+1}\"):\n",
    "        full_params = {**params, \"subsample\": 0.8, \"colsample_bytree\": 0.8,\n",
    "                       \"random_state\": 42, \"n_jobs\": -1}\n",
    "\n",
    "        model_lgbm = LGBMClassifier(**full_params)\n",
    "        model_lgbm.fit(X_train_scaled, y_train)\n",
    "        y_pred = model_lgbm.predict(X_test_scaled)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1  = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "        mlflow.log_params(full_params)\n",
    "        mlflow.log_metric(\"accuracy\",    acc)\n",
    "        mlflow.log_metric(\"f1_weighted\", f1)\n",
    "\n",
    "        tuning_results.append({\"config\": f\"Config {i+1}\", \"accuracy\": acc, \"f1\": f1, **params})\n",
    "        print(f\"Config {i+1} ‚Üí Accuracy: {acc:.4f} | F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 4 : R√©sultats du tuning manuel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df_tuning = pd.DataFrame(tuning_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Graphique accuracy\n",
    "colors_tuning = ['#3498db', '#e67e22', '#2ecc71']\n",
    "bars = axes[0].bar(df_tuning['config'], df_tuning['accuracy'],\n",
    "                   color=colors_tuning, alpha=0.85)\n",
    "for bar, val in zip(bars, df_tuning['accuracy']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                f'{val:.2%}', ha='center', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Tuning manuel ‚Äî Accuracy par configuration', fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(min(df_tuning['accuracy']) - 0.01, max(df_tuning['accuracy']) + 0.02)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Tableau r√©capitulatif\n",
    "axes[1].axis('off')\n",
    "table_data = []\n",
    "for _, row in df_tuning.iterrows():\n",
    "    table_data.append([\n",
    "        row['config'],\n",
    "        str(int(row['n_estimators'])),\n",
    "        str(int(row['max_depth'])),\n",
    "        str(row['learning_rate']),\n",
    "        f\"{row['accuracy']:.2%}\"\n",
    "    ])\n",
    "table = axes[1].table(\n",
    "    cellText=table_data,\n",
    "    colLabels=['Config', 'n_estimators', 'max_depth', 'learning_rate', 'Accuracy'],\n",
    "    loc='center', cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 2)\n",
    "axes[1].set_title('Param√®tres test√©s', fontweight='bold')\n",
    "\n",
    "plt.suptitle('R√©sultats du Tuning Manuel LightGBM', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tuning_manual_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìé √âTAPE 9 ‚Äî Artefacts suppl√©mentaires du meilleur mod√®le\n",
    "\n",
    "On logue des artefacts suppl√©mentaires sur la meilleure configuration :\n",
    "1. **Matrice de confusion** (PNG)\n",
    "2. **Feature names** (TXT) ‚Äî liste des colonnes utilis√©es\n",
    "3. **M√©tadonn√©es du dataset** ‚Äî taille, split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"tuning-lightgbm\")\n",
    "\n",
    "best_params = {\"n_estimators\": 200, \"max_depth\": 8, \"learning_rate\": 0.05,\n",
    "               \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"random_state\": 42, \"n_jobs\": -1}\n",
    "\n",
    "with mlflow.start_run(run_name=\"LightGBM-best-artefacts\"):\n",
    "    model_best = LGBMClassifier(**best_params)\n",
    "    model_best.fit(X_train_scaled, y_train)\n",
    "    y_pred_best = model_best.predict(X_test_scaled)\n",
    "\n",
    "    report = classification_report(y_test, y_pred_best, output_dict=True)\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"accuracy\",    accuracy_score(y_test, y_pred_best))\n",
    "    mlflow.log_metric(\"f1_weighted\", report[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "    # Artefact 1 : Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_best)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax, cmap=\"Blues\")\n",
    "    ax.set_title(\"Confusion Matrix ‚Äî LightGBM Best\")\n",
    "    plt.savefig(\"confusion_matrix_best.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix_best.png\")  # Envoi dans MLflow\n",
    "    plt.close()\n",
    "\n",
    "    # Artefact 2 : Feature names\n",
    "    with open(\"feature_names.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(list(X.columns)))\n",
    "    mlflow.log_artifact(\"feature_names.txt\")  # Envoi dans MLflow\n",
    "\n",
    "    # Artefact 3 : M√©tadonn√©es dataset\n",
    "    mlflow.log_param(\"dataset_rows\", len(df_accidents))\n",
    "    mlflow.log_param(\"dataset_cols\", X.shape[1])\n",
    "    mlflow.log_param(\"train_size\",   len(X_train))\n",
    "    mlflow.log_param(\"test_size\",    len(X_test))\n",
    "\n",
    "    print(f\"‚úÖ Accuracy : {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "    print(\"‚úÖ Artefacts logg√©s : confusion matrix, feature names, m√©tadonn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üóÑÔ∏è √âTAPE 10 ‚Äî Model Registry\n",
    "\n",
    "Le **Model Registry** est l'entrep√¥t versionn√© de MLflow. Le mod√®le XGBoost a √©t√© enregistr√© lors de la boucle d'entra√Ænement.\n",
    "\n",
    "On peut le recharger directement pour faire des pr√©dictions ‚Äî sans r√©entra√Æner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechargement du mod√®le XGBoost depuis le Model Registry\n",
    "model_name_reg = \"XGBoost_accidents_model\"\n",
    "model_uri      = f\"models:/{model_name_reg}/latest\"  # 'latest' = derni√®re version\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "predictions  = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"‚úÖ Mod√®le recharg√© depuis le Registry\")\n",
    "print(f\"5 premi√®res pr√©dictions : {predictions[:5]}\")\n",
    "print(\"1=Indemne | 2=Bless√© l√©ger | 3=Hospitalis√© | 4=Tu√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç √âTAPE 11 ‚Äî GridSearchCV + MLflow\n",
    "\n",
    "**GridSearchCV** teste automatiquement **toutes les combinaisons** d'une grille de param√®tres.\n",
    "\n",
    "Exemple : 2 valeurs √ó 2 valeurs √ó 2 valeurs = **8 combinaisons** test√©es automatiquement.\n",
    "\n",
    "**cv=3** = cross-validation √† 3 folds : chaque combinaison est test√©e 3 fois sur des portions diff√©rentes. Total : 8 √ó 3 = **24 entra√Ænements**.\n",
    "\n",
    "**Avantage** : exhaustif, ne rate aucune combinaison.  \n",
    "**Inconv√©nient** : lent si la grille est grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"gridsearch-lightgbm\")\n",
    "\n",
    "# Grille des param√®tres √† tester ‚Äî toutes les combinaisons seront test√©es\n",
    "param_grid = {\n",
    "    \"n_estimators\":  [100, 200],   # 2 valeurs\n",
    "    \"max_depth\":     [6, 8],       # 2 valeurs\n",
    "    \"learning_rate\": [0.05, 0.01]  # 2 valeurs ‚Üí 8 combinaisons\n",
    "}\n",
    "\n",
    "base_model = LGBMClassifier(subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,              # 3-fold cross-validation\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Logger chaque combinaison comme un run MLflow s√©par√©\n",
    "gridsearch_results = []\n",
    "for i in range(len(grid_search.cv_results_[\"params\"])):\n",
    "    with mlflow.start_run(run_name=f\"GridSearch-combo-{i+1}\"):\n",
    "        mlflow.log_params(grid_search.cv_results_[\"params\"][i])\n",
    "        score = grid_search.cv_results_[\"mean_test_score\"][i]\n",
    "        mlflow.log_metric(\"mean_cv_accuracy\", score)\n",
    "        gridsearch_results.append({**grid_search.cv_results_[\"params\"][i], \"score\": score})\n",
    "\n",
    "print(f\"\\n‚úÖ GridSearch termin√©\")\n",
    "print(f\"Meilleurs param√®tres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleure accuracy   : {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 5 : R√©sultats GridSearch ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df_gs = pd.DataFrame(gridsearch_results).sort_values('score', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "labels  = [f\"n={r['n_estimators']}\\nd={r['max_depth']}\\nlr={r['learning_rate']}\" for _, r in df_gs.iterrows()]\n",
    "colors_gs = ['#2ecc71' if i == 0 else '#3498db' for i in range(len(df_gs))]\n",
    "bars = ax.bar(range(len(df_gs)), df_gs['score'], color=colors_gs, alpha=0.85)\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, df_gs['score'])):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "            f'{val:.3f}', ha='center', fontsize=9, fontweight='bold' if i == 0 else 'normal')\n",
    "\n",
    "ax.set_xticks(range(len(df_gs)))\n",
    "ax.set_xticklabels(labels, fontsize=8)\n",
    "ax.set_ylabel('Accuracy (CV moyenne)')\n",
    "ax.set_title('GridSearchCV ‚Äî Comparaison des 8 combinaisons\\n(vert = meilleure combinaison)',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(min(df_gs['score']) - 0.005, max(df_gs['score']) + 0.01)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gridsearch_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† √âTAPE 12 ‚Äî Optuna + MLflow\n",
    "\n",
    "**Optuna** utilise l'**optimisation bay√©sienne** : il apprend de chaque essai pour cibler les prochains param√®tres les plus prometteurs. Avec seulement 10 essais, il trouve souvent mieux que GridSearch avec 24 essais.\n",
    "\n",
    "**Analogie** : au lieu de go√ªter les 500 caf√©s de Lyon dans l'ordre, Optuna go√ªte 5 caf√©s, apprend ce qu'on aime, et cible intelligemment les suivants.\n",
    "\n",
    "**`trial.suggest_int / suggest_float`** : Optuna choisit intelligemment une valeur dans la plage indiqu√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "mlflow.set_experiment(\"optuna-lightgbm\")\n",
    "\n",
    "optuna_results = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna choisit intelligemment les valeurs dans ces plages\n",
    "    params = {\n",
    "        \"n_estimators\":     trial.suggest_int(  \"n_estimators\",  100, 500),\n",
    "        \"max_depth\":        trial.suggest_int(  \"max_depth\",     4,   12),\n",
    "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        \"subsample\":        0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"random_state\":     42,\n",
    "        \"n_jobs\":           -1\n",
    "    }\n",
    "\n",
    "    # Chaque essai Optuna = 1 run MLflow\n",
    "    with mlflow.start_run(run_name=f\"Optuna-trial-{trial.number}\"):\n",
    "        model_opt = LGBMClassifier(**params)\n",
    "        model_opt.fit(X_train_scaled, y_train)\n",
    "        y_pred = model_opt.predict(X_test_scaled)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "        optuna_results.append({\"trial\": trial.number, \"accuracy\": acc, **params})\n",
    "\n",
    "    return acc  # Optuna utilise ce score pour orienter le prochain essai\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(f\"\\n‚úÖ Optuna termin√© ‚Äî 10 essais intelligents\")\n",
    "print(f\"Meilleurs param√®tres : {study.best_params}\")\n",
    "print(f\"Meilleure accuracy   : {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 6 : Progression d'Optuna (apprentissage au fil des essais) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df_optuna = pd.DataFrame(optuna_results)\n",
    "best_so_far = df_optuna['accuracy'].cummax()  # Meilleur score √† chaque essai\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Progression du meilleur score\n",
    "axes[0].plot(df_optuna['trial'], df_optuna['accuracy'],\n",
    "             'o-', color='#3498db', alpha=0.6, label='Score de l\\'essai')\n",
    "axes[0].plot(df_optuna['trial'], best_so_far,\n",
    "             's--', color='#2ecc71', linewidth=2, label='Meilleur score cumul√©')\n",
    "axes[0].set_xlabel('Num√©ro d\\'essai')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Progression d\\'Optuna au fil des essais', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Relation learning_rate vs accuracy\n",
    "scatter = axes[1].scatter(\n",
    "    df_optuna['learning_rate'], df_optuna['accuracy'],\n",
    "    c=df_optuna['n_estimators'], cmap='viridis', s=100, alpha=0.8\n",
    ")\n",
    "plt.colorbar(scatter, ax=axes[1], label='n_estimators')\n",
    "axes[1].set_xlabel('Learning Rate')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Impact du learning_rate sur l\\'accuracy\\n(couleur = n_estimators)', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Analyse des r√©sultats Optuna', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('optuna_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã √âTAPE 13 ‚Äî Bilan final et comparaison des 3 m√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Graphique 7 : Comparaison des 3 m√©thodes de tuning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "best_base     = max(results[m]['accuracy'] for m in results)\n",
    "best_manual   = max(r['accuracy'] for r in tuning_results)\n",
    "best_grid     = grid_search.best_score_\n",
    "best_optuna   = study.best_value\n",
    "\n",
    "methods  = ['Base\\n(4 mod√®les)', 'Tuning\\nManuel', 'GridSearchCV', 'Optuna']\n",
    "scores   = [best_base, best_manual, best_grid, best_optuna]\n",
    "colors_f = ['#95a5a6', '#3498db', '#e67e22', '#2ecc71']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(methods, scores, color=colors_f, alpha=0.85, width=0.5)\n",
    "\n",
    "for bar, val in zip(bars, scores):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "            f'{val:.2%}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Meilleure Accuracy')\n",
    "ax.set_title('Progression des performances ‚Äî Du mod√®le de base au tuning avanc√©',\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(min(scores) - 0.05, max(scores) + 0.03)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Fl√®ches de progression\n",
    "for i in range(len(scores) - 1):\n",
    "    gain = scores[i+1] - scores[i]\n",
    "    ax.annotate(\n",
    "        f'+{gain:.2%}',\n",
    "        xy=(i + 0.75, (scores[i] + scores[i+1]) / 2),\n",
    "        fontsize=10, color='darkgreen', fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(\"BILAN FINAL\")\n",
    "print(\"=\"*55)\n",
    "print(f\"{'M√©thode':<20} {'Meilleure accuracy':<20} {'Nb essais'}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Base (LightGBM)':<20} {best_base:<20.2%} 1\")\n",
    "print(f\"{'Tuning manuel':<20} {best_manual:<20.2%} 3 (choisis √† la main)\")\n",
    "print(f\"{'GridSearchCV':<20} {best_grid:<20.2%} 8 (grille compl√®te)\")\n",
    "print(f\"{'Optuna':<20} {best_optuna:<20.2%} 10 (intelligents)\")\n",
    "print(\"=\"*55)\n",
    "print(\"\\n‚Üí Optuna trouve le meilleur r√©sultat avec le moins d'essais\")\n",
    "print(\"‚Üí Probl√®me principal : d√©s√©quilibre de la classe 2 (Bless√© l√©ger)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ √âTAPE 14 ‚Äî Sauvegarde du mod√®le final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du mod√®le au format pickle\n",
    "try:\n",
    "    joblib.dump(model_best, \"model.pkl\")\n",
    "    print(\"‚úÖ Mod√®le sauvegard√© : model.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {e}\")\n",
    "\n",
    "# Sauvegarde des noms de classes\n",
    "class_names = {\"1\": \"Indemne\", \"2\": \"Bless√© l√©ger\", \"3\": \"Hospitalis√©\", \"4\": \"Tu√©\"}\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f, ensure_ascii=False, indent=2)\n",
    "print(\"‚úÖ Classes sauvegard√©es : class_names.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Conclusion\n",
    "\n",
    "### Ce qui a √©t√© accompli\n",
    "\n",
    "| √âtape | Description | R√©sultat |\n",
    "|-------|-------------|----------|\n",
    "| Jour 1 | Comparaison de 4 mod√®les | LightGBM meilleur (~66%) |\n",
    "| Jour 1 | Tracking MLflow complet | Params + m√©triques + artefacts logg√©s |\n",
    "| Jour 2 | Tuning manuel (3 configs) | +~3% d'accuracy |\n",
    "| Jour 2 | Artefacts suppl√©mentaires | Confusion matrix, feature names |\n",
    "| Jour 2 | Model Registry | XGBoost versionn√© et rechargeable |\n",
    "| Jour 3 | GridSearchCV (8 combos) | Confirme la meilleure config |\n",
    "| Jour 3 | Optuna (10 essais) | Meilleur r√©sultat avec moins d'essais |\n",
    "\n",
    "### Piste d'am√©lioration principale\n",
    "\n",
    "La **classe 2 (Bless√© l√©ger)** est tr√®s mal pr√©dite par tous les mod√®les.  \n",
    "Cause : d√©s√©quilibre des classes dans le dataset.  \n",
    "Solution : `SMOTE` (oversampling synth√©tique) ou ajustement du `class_weight`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}